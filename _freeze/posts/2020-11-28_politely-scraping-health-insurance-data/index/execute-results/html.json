{
  "hash": "7891fab7f454c0900789951723e283e4",
  "result": {
    "markdown": "---\ntitle: \"Politely Scraping Health Insurance Data\"\ndate: \"2020-11-28\"\ncategories: [webscraping, rvest, polite, gganimate, gt]\nimage: \"meps-banner.PNG\"\nexecute: \n  warning: false\n  output: true\n---\n\n\n\n\n\n\n![](meps-banner.PNG){style=\"border: 5px solid #555;\" fig-alt=\"Screenshot of MEPS webpage table with data on private-sector establishments\"}\n\n## Health insurance benchmark data\n\nIn the United States, about [half](https://www.kff.org/other/state-indicator/total-population/?currentTimeframe=0&sortModel=%7B%22colId%22:%22Location%22,%22sort%22:%22asc%22%7D) of the population has health insurance coverage through an employer. With employer-sponsored health insurance having such a large role in the American health care system, it's important to understand trends and variation over time and how that affects affordability for employers, workers, and their family members.\n\nOne of the longest running surveys of employer-sponsored coverage is the [Medical Expenditure Panel Survey - Insurance Component (MEPS-IC)](https://meps.ahrq.gov/mepsweb/survey_comp/Insurance.jsp), administered by the federal Agency for Healthcare Research and Quality. Since the late 1990s, the MEPS-IC has asked employees and their employers about the health benefits offered to employees and what benefits employees enroll into, making it a valuable source of benchmark and trend data.\n\nA significant challenge with MEPS-IC data is that it is stored in [separate web pages](https://meps.ahrq.gov/mepsweb/data_stats/quick_tables_results.jsp?component=2&subcomponent=1&year=-1&tableSeries=-1&tableSubSeries=&searchText=&searchMethod=1&Action=Search) for each measure and year, with [separate point estimate and standard error tables](https://meps.ahrq.gov/data_stats/summ_tables/insr/national/series_1/2019/tia1.htm) on each page. For this project, I am going to scrape the data from a subset of MEPS-IC tables using appropriate techniques and then visualize the trend data.\n\nLet's get to it and load the R packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(rvest)\n#devtools::install_github(\"dmi3kno/polite\")\nlibrary(polite)\nlibrary(httr)\nlibrary(gganimate)\nlibrary(sf)\nlibrary(viridis)\nlibrary(gt)\n#devtools::install_github(\"hadley/emo\")\nlibrary(emo)\n```\n:::\n\n\n## Web scraping and the `polite` package\n\nTo scrape the data from the MEPS-IC website, I'm going to use the [`polite` package](https://github.com/dmi3kno/polite), developed by Dmytro Perepolkin and co-authors. This package uses three principles of polite webscraping: *seeking permission, taking slowly and never asking twice*. Specifically, it manages the http session, declares the user agent string and checks the site policies, and uses rate-limiting and response caching to minimize the impact on the webserver.\n\nApplying the three principles of polite scraping, `polite` creates a session with `bow`, requests a page with `nod`, and pulls the contents of the page with `scrape`. Here is a brief example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsession <- bow(\"https://meps.ahrq.gov/\", force = TRUE)\nsession\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<polite session> https://meps.ahrq.gov/\n    User-agent: polite R package\n    robots.txt: 13 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n:::\n\n```{.r .cell-code}\npage <- \n    nod(bow = session, \n        path = paste0(\"data_stats/summ_tables/insr/national/\", \n                      \"series_1/2019/tia1.htm\"))\npage\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<polite session> https://meps.ahrq.gov/data_stats/summ_tables/insr/national/series_1/2019/tia1.htm\n    User-agent: polite R package\n    robots.txt: 13 rules are defined for 1 bots\n   Crawl delay: 5 sec\n  The path is scrapable for this user-agent\n```\n:::\n\n```{.r .cell-code}\nscrape_page <- \n  page %>% \n    scrape(verbose=TRUE)\nscrape_page\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_document}\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n[1] <head>\\n<meta name=\"description\" content=\"2019 Insurance Component Summar ...\n[2] <body>\\r\\n<center>\\r\\n<table cellspacing=\"0\" border=\"0\" cellpadding=\"3\" c ...\n```\n:::\n:::\n\n\n## Creating a table of contents of the insurance data\n\nThe first step for scraping the MEP-IC pages is to generate a table of content of all of the webpages. To do this, I use `polite` to go through each of the pages in the MEPS-IC table of contents and append the results of each page together, using a while loop. I also use the `rvest` package to pull specific nodes from each page, including the table and a link to the next page.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Table of contents\ntoc_df <- tibble()\nsession <- bow(\"https://meps.ahrq.gov/\", force = TRUE)\n\n## State and metro tables\npath <- paste0(\"quick_tables_results.jsp?component=2&subcomponent=2\", \n               \"&year=-1&tableSeries=-1&tableSubSeries=&searchText=&\",\n               \"searchMethod=1&Action=Search\")\nindex <- 0\n\nwhile(!is.na(path)){\n  # make it verbose\n  index <- 1 + index\n  message(\"Scraping state/metro page: \", path)\n  # nod and scrape\n  current_page <- \n    nod(bow = session, \n        path = paste0(\"data_stats/\", path)) %>% \n    scrape(verbose=TRUE)\n  # extract post titles\n  toc_df <- current_page %>%\n    html_nodes(xpath = paste0('//*[contains(concat( \" \", @class, \" \" ),',\n                              ' concat( \" \", \"description\", \" \" ))]',\n                              '//table')) %>%\n    html_table(fill = TRUE) %>%\n    purrr::pluck(5) %>% \n    as_tibble() %>% \n    janitor::clean_names() %>% \n    bind_rows(toc_df)\n  # see if a \"Next\" link is available\n  if (index == 1){\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder div a\") %>% \n      html_attr(\"href\")\n  } else {\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder a+ a\") %>% \n      html_attr(\"href\")\n  }\n} # end while loop\n\n## National tables\npath <- paste0(\"quick_tables_results.jsp?component=2&subcomponent=1&\",\n               \"year=-1&tableSeries=-1&tableSubSeries=&searchText=&\", \n               \"searchMethod=1&Action=Search\")\nindex <- 0\n\nwhile(!is.na(path)){\n  # update index\n  index <- 1 + index\n  message(\"Scraping national page: \", path)\n  # nod and scrape\n  current_page <- \n    nod(bow = session, \n        path = paste0(\"data_stats/\", path)) %>% \n    scrape(verbose=TRUE)\n  # extract TOC tables\n  toc_df <- current_page %>%\n    html_nodes(xpath = paste0('//*[contains(concat( \" \", @class, \" \" ),',\n                              'concat( \" \", \"description\", \" \" ))]//table')) %>%\n    html_table(fill = TRUE) %>%\n    purrr::pluck(5) %>% \n    as_tibble() %>% \n    janitor::clean_names() %>% \n    bind_rows(toc_df)\n  # see if a \"Next\" link is available\n  if (index == 1){\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder div a\") %>% \n      html_attr(\"href\")\n  } else {\n    path <- current_page %>% \n      html_node(\".boxGreyNoBorder a+ a\") %>% \n      html_attr(\"href\")\n  }\n  }# end while loop\n```\n:::\n\n\nAfter scraping the table of contents data, I prep it further and generate the set of URL links I will need to pull the specific data tables I plan to use for this project.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntoc <- \n  toc_df %>% \n  select(-update) %>% \n  separate(title,\n           into = c(\"title\", \"year\"),\n           sep = \": United States,\",\n           convert = TRUE) %>% \n  mutate(year = as.numeric(substr(str_squish(year), 1, 4)),\n         number = str_remove(table_no, \"Table \")) %>% \n  separate(number,\n           into = c(\"series\", \"number\"),\n           sep = '\\\\.',\n           convert = TRUE,\n           extra = \"merge\") %>% \n  filter(!is.na(year)) %>% \n  mutate(number = str_remove_all(number, \"[:punct:]\"),\n         number = str_to_lower(paste0(\"t\", series, number)),\n         series = as.numeric(as.roman(series)),\n         url = case_when(\n           series %in% c(1, 3, 4, 11) ~ paste0(\"https://meps.ahrq.gov/data_stats/\",\n                                               \"summ_tables/insr/national/series_\",\n                                               series, \"/\", \n                                               year, \"/\", \n                                               number, \".htm\"),\n           TRUE ~ paste0(\"https://meps.ahrq.gov/data_stats/\", \n                         \"summ_tables/insr/state/series_\", \n                         series, \"/\", \n                         year, \"/\", \n                         number, \".htm\")\n         )) %>% \n  filter(!is.na(year) & !is.na(series) & !is.na(number))\n```\n:::\n\n\n## Scraping data on family premium costs\n\nAs part of the project, I'm interested in the trends in the total premium costs of family health insurance through an employer (the sum of employer and employee contributions). To pull this data, I need to scrape the data from [Table IID1](https://meps.ahrq.gov/data_stats/summ_tables/insr/state/series_2/2019/tiid1.htm), which has this data by state. First, I pull a vector of URLs from the table of contents, and then I use the `politely` wrapper function to apply polite principles with `purrr::map` and the `httr::GET` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Family premiums by state\nfamily_prem_url_vctr <- \n  toc %>% \n  filter(number == \"tiid1\" & year >= 2000) %>% \n  pull(url)\n\n## Politely GET pages\npolite_GET <- politely(GET, verbose = FALSE)\n\nfamily_prem_res <- map(family_prem_url_vctr, polite_GET)\n```\n:::\n\n\n## Tidying the scraped data with the tidyverse\n\nThe result is a list object with a length of the number of pages scraped.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(family_prem_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"list\"\n```\n:::\n\n```{.r .cell-code}\nlength(family_prem_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 19\n```\n:::\n:::\n\n\nWith a little bit of `rvest` and `purrr` you can see a preview of the table in the webpage, but the results are very messy, with extra columns, blank rows, and offset values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfamily_prem_res %>% \n  purrr::pluck(1) %>% \n  read_html() %>% \n  html_nodes(xpath = '/html/body/table[1]') %>% \n  html_table(fill = TRUE, header = FALSE) %>%\n  as.data.frame() %>% \n  tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 59 x 18\n   X1    X2    X3    X4    X5    X6    X7    X8    X9    X10   X11   X12   X13  \n   <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~ \"Tab~\n 2 \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 3 \"Div~ \"Tot~ \"\"    \"Les~ \"\"    \"10 ~ \"\"    \"25 ~ \"\"    \"100~ \"\"    \"100~ \"\"   \n 4 \"Div~ \"Tot~ \"\"    \"Les~ \"\"    \"10 ~ \"\"    \"25 ~ \"\"    \"100~ \"\"    \"100~ \"\"   \n 5 \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 6 \"Uni~ \"6,7~ \"\"    \"6,9~ \"\"    \"6,8~ \"\"    \"6,6~ \"\"    \"6,6~ \"\"    \"6,8~ \"\"   \n 7 \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~ \"New~\n 8 \"Mas~ \"7,3~ \"\"    \"8,4~ \"\"    \"8,2~ \"\"    \"7,4~ \"\"    \"7,0~ \"\"    \"7,0~ \"\"   \n 9 \"New~ \"7,5~ \"\"    \"8,2~ \"\"    \"7,3~ \"\"    \"7,7~ \"\"    \"6,9~ \"\"    \"7,6~ \"\"   \n10 \"Con~ \"7,2~ \"\"    \"7,5~ \"\"    \"7,6~ \"\"    \"7,1~ \"\"    \"7,9~ \"\"    \"7,0~ \"\"   \n# ... with 49 more rows, and 5 more variables: X14 <chr>, X15 <chr>, X16 <chr>,\n#   X17 <chr>, X18 <chr>\n```\n:::\n:::\n\n\nTo fix the table and transform it to a shape that is better for visualization, I created a function that processes each scraped result and puts it into a tidier format (long, not wide). Some highlights of this function include:\n\n-   Testing different xpaths to find the data table in the page. Over time, the MEPS-IC tables changed in their format and styling, so the pages do not have a consistent structure.\n\n-   Apply a hierarchy of group categories, groups, and segments.\n\n-   Pull the point estimate and standard error data tables separately and combine them at the end.\n\n-   Use the `janitor` package to remove empty rows and columns and rename variables from row values.\n\n-   Use the `zoo` package to fill in blank values from above rows.\n\n-   Convert all percentages to decimal values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseriesScraper <- function(resp) {\n\n  if (resp %>%\n      purrr::pluck(1) %>% \n      read_html() %>% #test to find the right node\n      html_nodes(xpath = '/html/body/center/table[1]') %>%\n      html_table(fill = TRUE, header = FALSE) %>%\n      as.data.frame() %>%\n      ncol() > 0) {\n    node <- \"/html/body/center/table\"\n  } else if (resp %>%\n             purrr::pluck(1) %>% \n             read_html() %>% #test to find the right node\n             html_nodes(xpath = '/html/body/table[1]') %>%\n             html_table(fill = TRUE, header = FALSE) %>%\n             as.data.frame() %>%\n             ncol() > 0) {\n    node <- \"/html/body/table\"\n  } else {\n    node <- \"/html/body/div/table\"\n  }\n  \n  string_filter <- paste(\n    \"\\\\_\\\\_\",\n    \"Source:\",\n    \"Note:\",\n    \"Table I\",\n    \"Table II\",\n    \"Table 1\",\n    \"Table 2\",\n    \"\\\\* Figure\",\n    \"\\\\*\\\\* Definitions\",\n    \"Totals may\",\n    \"Dollar amounts\",\n    \"\\\\*Figure\",\n    \"Definitions\",\n    \"No estimate\",\n    \"These cell\",\n    \"States not\",\n    sep = \"|\"\n  )\n  \n  url <- \n    resp %>% \n    purrr::pluck(1) %>% \n    purrr::pluck(\"url\")\n  \n  if (is.null(url)){\n    url <- \n      resp %>% \n      purrr::pluck(1)\n  }\n  \n  pt_ests <-\n    resp %>%\n    purrr::pluck(1) %>% \n    read_html() %>%\n    html_nodes(xpath = paste0(node, \"[1]\")) %>%\n    html_table(fill = TRUE, header = FALSE) %>%\n    as.data.frame() %>%\n    na_if(\"\") %>%\n    distinct() %>%\n    filter(X1 != \"District of Columbia\" &\n             X2 != \"District of Columbia\") %>%\n    mutate_at(vars(-X1), ~ ifelse(!is.na(X1) &\n                                    is.na(.), X1, .)) %>%\n    mutate(X1 = ifelse(is.na(X1) & !is.na(X2), \"group\", X1)) %>%\n    janitor::remove_empty(\"rows\") %>%\n    mutate_all( ~ str_replace_all(., \"[\\r\\n]\" , \"\")) %>%\n    mutate_all( ~ str_squish(str_remove(., \"\\\\*\\\\*\"))) %>%\n    mutate_all( ~ replace(., str_detect(., \"These cell\"), NA)) %>%\n    filter(!str_detect(.[[1]], string_filter)) %>%\n    janitor::remove_empty(c(\"rows\", \"cols\")) %>%\n    distinct() %>%\n    janitor::row_to_names(\n      row_number = 1,\n      remove_row = TRUE,\n      remove_rows_above = TRUE\n    ) %>%\n    subset(select = which(!duplicated(names(.)))) %>%\n    subset(select = which(!is.na(names(.)))) %>%\n    mutate(\n      group_category = ifelse(.[[1]] == .[[2]], .[[1]], NA),\n      group_category = zoo::na.locf(group_category, na.rm = FALSE),\n      group_category = ifelse(is.na(group_category), \"United States\", group_category)\n    ) %>%\n    filter(!.[[1]] == .[[2]]) %>%\n    rename(group = 1) %>%\n    pivot_longer(\n      cols = -starts_with(\"group\"),\n      names_to = \"segment\",\n      values_to = \"pt_est\"\n    ) %>%\n    mutate_if(is.factor, as.character)\n  \n  std_errs <- \n    resp %>%\n    purrr::pluck(1) %>% \n    read_html() %>%\n    html_nodes(xpath = paste0(node, \"[2]\")) %>%\n    html_table(fill = TRUE, header = FALSE) %>%\n    as.data.frame() %>%\n    na_if(\"\") %>%\n    distinct() %>%\n    filter(X1 != \"District of Columbia\" &\n             X2 != \"District of Columbia\") %>%\n    mutate_at(vars(-X1), ~ ifelse(!is.na(X1) &\n                                    is.na(.), X1, .)) %>%\n    mutate(X1 = ifelse(is.na(X1) & !is.na(X2), \"group\", X1)) %>%\n    janitor::remove_empty(\"rows\") %>%\n    mutate_all( ~ str_replace_all(., \"[\\r\\n]\" , \"\")) %>%\n    mutate_all( ~ str_squish(str_remove(., \"\\\\*\\\\*\"))) %>%\n    mutate_all( ~ replace(., str_detect(., \"These cell\"), NA)) %>%\n    filter(!str_detect(.[[1]], string_filter)) %>%\n    janitor::remove_empty(c(\"rows\", \"cols\")) %>%\n    distinct() %>%\n    janitor::row_to_names(\n      row_number = 1,\n      remove_row = TRUE,\n      remove_rows_above = TRUE\n    ) %>%\n    subset(select = which(!duplicated(names(.)))) %>%\n    subset(select = which(!is.na(names(.)))) %>%\n    mutate(\n      group_category = ifelse(.[[1]] == .[[2]], .[[1]], NA),\n      group_category = zoo::na.locf(group_category, na.rm = FALSE),\n      group_category = ifelse(is.na(group_category), \"United States\", group_category)\n    ) %>%\n    filter(!.[[1]] == .[[2]]) %>%\n    rename(group = 1) %>%\n    pivot_longer(\n      cols = -starts_with(\"group\"),\n      names_to = \"segment\",\n      values_to = \"std_err\"\n    ) %>%\n    mutate_if(is.factor, as.character) %>% \n    select(-group_category)\n  \n  inner_join(pt_ests,\n             std_errs,\n             by = c(\"group\", \"group_category\", \"segment\")) %>%\n    mutate(url = url) %>%\n    mutate(numeric = case_when(\n      str_detect(pt_est, \"[A-Za-z]\") ~ 0,\n      str_detect(std_err, \"[A-Za-z]\") ~ 0,\n      TRUE ~ 1\n    )) %>%\n    filter(numeric == 1) %>%\n    mutate(\n      pt_est = str_trim(str_remove_all(pt_est, \"\\\\*\"), side = \"both\"),\n      std_err = str_trim(str_remove_all(std_err, \"\\\\*\"), side = \"both\"),\n      pt_est = if_else(\n        str_detect(pt_est, \"%\"),\n        as.numeric(gsub(\"%\", \"\", pt_est)) / 100,\n        as.numeric(gsub(\",\", \"\", pt_est))\n      ),\n      std_err = if_else(\n        str_detect(std_err, \"%\"),\n        as.numeric(gsub(\"%\", \"\", std_err)) / 100,\n        as.numeric(gsub(\",\", \"\", std_err))\n      )\n    ) %>%\n    select(-numeric) %>%\n    janitor::remove_empty(c(\"cols\", \"rows\")) %>% \n    mutate(\n      series = parse_number(str_extract(url, \"series_[:digit:]{1,2}\")),\n      year = parse_number(str_extract(url, \"[:digit:]{4}\")),\n      table = str_remove(str_extract(url, \"[:alnum:]{2,12}.htm\"), \".htm\")\n    ) %>% \n    filter(!is.na(pt_est) & !is.na(std_err)) %>%\n    mutate_if(is.character, ~ str_remove(., \"\\\\:$\"))\n}\n```\n:::\n\n\nWith this new function, I used `purrr::map_dfr` to apply it to each scraped page in the list object and append all the resulting tidy data frames together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfamily_prem_df <- map_dfr(family_prem_res, seriesScraper)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(family_prem_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 9\n  group         group_category segment   pt_est std_err url   series  year table\n  <chr>         <chr>          <chr>      <dbl>   <dbl> <chr>  <dbl> <dbl> <chr>\n1 United States United States  Total      6772.    19.6 http~      2  2000 tiid1\n2 United States United States  Less tha~  6994.   149   http~      2  2000 tiid1\n3 United States United States  10 - 24 ~  6860.   143.  http~      2  2000 tiid1\n4 United States United States  25 - 99 ~  6628.    78.4 http~      2  2000 tiid1\n5 United States United States  100-999 ~  6606.    52.3 http~      2  2000 tiid1\n6 United States United States  1000 or ~  6817.    37.7 http~      2  2000 tiid1\n```\n:::\n:::\n\n\n## Visualizing family premium costs\n\nFor the family premium cost data, I want to generate a map to show how costs increase across states over time. For the state map, I'm going with a hexmap of states from [Carto](https://team.carto.com/u/andrew/tables/us_states_hexgrid/public#). To make all the hexagons the same size, I transformed the data to a Mercator projection. I also created a crosswalk of state names and abbreviations to join the data together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhex_map <- st_read(file.path(path_to_data, \"us_states_hexgrid.geojson\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `us_states_hexgrid' from data source \n  `C:\\Users\\Josh\\Dropbox\\Projects\\meps-ic\\polite\\us_states_hexgrid.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 51 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -137.9747 ymin: 26.39343 xmax: -69.90286 ymax: 55.3132\nGeodetic CRS:  WGS 84\n```\n:::\n\n```{.r .cell-code}\nstate_abb <- \n  tibble(\n    name = state.name,\n    abb = state.abb\n  ) %>% \n  bind_rows(\n    tibble(\n      name = \"District of Columbia\",\n      abb = \"DC\"\n    )\n  )\n\n#transform to mercator projection\nhex_map_merc <- st_transform(hex_map, crs = 3785)\n\nhex_map_merc %>% \n  ggplot() +\n  geom_sf()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/prep map file-1.png){width=672}\n:::\n:::\n\n\nThe next step is to prep the family premium data with the map. I used the `tidyr::complete` function to add missing values for any states that did not have data for a particular year. This is most common in the earlier years of the survey. I then used `gganimate` to created an animated gif showing the changes in premium costs across states.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfamily_prem_map_df <- \n  family_prem_df %>% \n  filter(segment == \"Total\") %>% \n  select(year, group, pt_est, std_err) %>% \n  tidyr::complete(year, group) %>% \n  inner_join(\n    state_abb, by = c(\"group\" = \"name\")\n  ) %>% \n  inner_join(\n    hex_map_merc, by = c(\"abb\" = \"iso3166_2\")\n  ) %>% \n  select(\n    abb, \n    year, \n    pt_est,\n    std_err,\n    geometry\n  )\n\nfamily_prem_anim <-\n  family_prem_map_df %>% \n  ggplot() +\n  geom_sf(aes(fill = pt_est, geometry = geometry)) +\n  scale_fill_viridis_c(\n    option = \"magma\", \n    na.value = \"grey50\",\n    aesthetics = \"fill\",\n    labels = scales::dollar\n  ) +\n  geom_sf_label(aes(label = abb, geometry = geometry), \n                color = \"black\", \n                size = 3, \n                label.padding = unit(0.15, \"lines\")) +\n  geom_text(aes(x = -8400000, y = 3200000, label = paste0(year)),\n            size = 8,\n            color = 'black') +\n  theme_bw() + \n  theme(plot.title = element_text(face = \"bold\", hjust = 0, size = 24),\n        plot.subtitle = element_text(size = 14),\n        panel.border = element_blank(),\n        panel.spacing = unit(3, \"lines\"),\n        panel.grid = element_blank(),\n        axis.ticks = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        legend.key.width = unit(4, \"lines\"),\n        legend.position = c(0.50, 0.85),\n        legend.direction = \"horizontal\",\n        legend.title = element_blank()) +\n  labs(\n    title = 'Family Health Insurance Premium Costs',\n    subtitle = 'For enrolled workers with employer-based coverage',\n    caption = 'data source: MEPS-IC | graphic by @joshfangmeier',\n    x = '',\n    y = ''\n  ) +\n  transition_states(year,\n                    transition_length = 4, state_length = 3) +\n  ease_aes('cubic-in-out')\n```\n:::\n\n\nYou can see which states have had higher and lower premium costs. Alaska ðŸ¤¯.\n\n![](family-prem.gif){fig-alt=\"Animated graphic showing the increase in family premiums by state from 2000 to 2019\"}\n\n## Scraping data on insurance coverage by industry\n\nThe next part of this project is to compare health insurance trends by industry. To do this, I scraped and tidied data from five sets of tables on the number of establishments, the number of workers, enrollment rates, and deductible levels for single (employee-only) health insurance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Coverage Rates by Industry\nenrollment_url_vctr <- \n  toc %>% \n  filter(number %in% c(\"tia1\", \"tib1\", \"tib2\", \"tib2b\", \"tif2\"),\n         year >= 2000) %>% \n  pull(url)\n\nenrollment_res <- map(enrollment_url_vctr, polite_GET)\n\nenrollment_df <- map_dfr(enrollment_res, seriesScraper)\n```\n:::\n\n\n## Comparing variation in deductibles across industries\n\nI then prepped the data a little further and calculated the number of enrolled employees for each industrial sector, using other values from the survey. I also plotted the trends in employee deductibles from 2000 to 2019 for each of the sectors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nenrollment_ind_df <- \n  enrollment_df %>% \n  filter(group_category == \"Industry group\" & segment == \"Total\") %>% \n  mutate(group = case_when(\n    group == \"Fin. svs. and real est.\" ~ \"Fin. svs. and real estate\",\n    group == \"Other Services\" ~ \"Other services\",\n    TRUE ~ group)) %>% \n  filter(group != \"Unknown\") %>% \n  select(year, group, pt_est, table, year) %>% \n  pivot_wider(names_from = table, values_from = pt_est) %>% \n  mutate(enrolled_employees = tib1 * tib2 * tib2b) %>% \n  select(year,\n         industry = group,\n         establishments = tia1,\n         employees = tib1,\n         enrolled_employees,\n         average_employee_deductible = tif2) %>% \n  filter(!is.na(average_employee_deductible))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nenrollment_ind_df %>% \n  ggplot(aes(x = year, y = average_employee_deductible)) +\n  geom_line() +\n  geom_hline(yintercept = 0.5) +\n  facet_wrap(~industry)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Creating a table to display industry variation\n\nInstead of creating another animated gif, I decided to use the `gt` package to generate a table that contained the information I wanted to display. Thomas Mock has an [excellent guide](https://themockup.blog/posts/2020-09-04-10-table-rules-in-r/) for creating high quality tables with `gt` that I highly recommend. He includes a [section](https://themockup.blog/posts/2020-09-04-10-table-rules-in-r/#sparklines) on how to implement plots in your table, so you can have numbers and visuals in your table together. In this case, I am going to create a line graph or spark plot to display the deductible trends, by creating a function to generate the plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_spark <- function(data){\n  data %>% \n    mutate(color = \"blue\") %>% \n    ggplot(aes(x = year, y = average_employee_deductible, color = color)) +\n    geom_line(size = 15) +\n    theme_void() +\n    scale_color_identity() +\n    theme(legend.position = \"none\")\n}\n\ndeductible_plots <- enrollment_ind_df %>% \n  select(industry, year, average_employee_deductible) %>% \n  nest(deductibles = c(year, average_employee_deductible)) %>% \n  mutate(plot = map(deductibles, plot_spark))\n```\n:::\n\n\nFor some added fun, I'm adding emojis that represent each of the industrial sectors. Hadley Wickham's [`emo` package](https://github.com/hadley/emo) makes it very easy to add emoji values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemoji <- tribble(\n  ~industry, ~emoji,\n  \"Agric., fish., forest.\", emo::ji(\"man_farmer\"),\n  \"Mining and manufacturing\", emo::ji(\"woman_factory_worker\"),\n  \"Construction\", emo::ji(\"construction_worker_man\"),\n  \"Utilities and transp.\", emo::ji(\"bus\"),\n  \"Wholesale trade\", emo::ji(\"ship\"),\n  \"Fin. svs. and real estate\", emo::ji(\"dollar\"),\n  \"Retail trade\", emo::ji(\"department_store\"),\n  \"Professional services\", emo::ji(\"woman_health_worker\"),\n  \"Other services\", emo::ji(\"man_artist\")\n)\n```\n:::\n\n\nUsing Mock's guide for `gt` tables, I then setup my new table with the embedded plots using the `text_transform` function and the data frame with the plots I created earlier. With `gt`, you can also easily add spanners, format the values in your columns, and change the font styling and background colors to create the look you want.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nenrollment_spark <-\n  enrollment_ind_df %>% \n  filter(year == 2019) %>% \n  inner_join(emoji, by = \"industry\") %>% \n  select(emoji, \n         industry, \n         establishments, \n         employees, \n         enrolled_employees, \n         average_employee_deductible) %>% \n  rename_all(~str_to_title(str_replace_all(., \"\\\\_\", \" \"))) %>% \n  mutate(ggplot = NA) %>% \n  gt() %>% \n  text_transform(\n    locations = cells_body(vars(ggplot)),\n    fn = function(x){\n      map(deductible_plots$plot, \n          ggplot_image, \n          height = px(20), \n          aspect_ratio = 4)\n    }\n  ) %>% \n  cols_width(vars(ggplot) ~ px(100)) %>% \n  cols_label(\n    ggplot = \"Employee Deductibles:\\n2000-2019\",\n    Emoji = \"\",\n  ) %>% \n  fmt_number(3:5, decimals = 0) %>% \n  fmt_currency(6, decimals = 0) %>% \n  tab_spanner(\n    label = \"2019 Private Sector Charactertics, USA\",\n    columns = c(3:6)\n  ) %>% \n  tab_style(\n    style = cell_text(color = \"black\", weight = \"bold\"),\n    locations = list(\n      cells_column_spanners(everything()),\n      cells_column_labels(everything())\n    )\n  ) %>%  \n  tab_options(\n    row_group.border.top.width = px(3),\n    row_group.border.top.color = \"black\",\n    row_group.border.bottom.color = \"black\",\n    table_body.hlines.color = \"white\",\n    table.border.top.color = \"white\",\n    table.border.top.width = px(3),\n    table.border.bottom.color = \"white\",\n    table.border.bottom.width = px(3),\n    column_labels.border.bottom.color = \"black\",\n    column_labels.border.bottom.width = px(2),\n  ) %>% \n  tab_source_note(md(\"**Source**: MEPS-IC | **Table by**: @joshfangmeier\"))\n\nenrollment_spark\n```\n:::\n\n\n![](meps-gt-table.png){fig-alt=\"Table showing the number of establishments, employees, enrolled employees, and deductibles by industry\"}\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}